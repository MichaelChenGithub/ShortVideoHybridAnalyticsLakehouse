version: '3.8'

services:
  # ---------------------------
  # 1. Storage Layer (MinIO)
  # ---------------------------
  minio:
    image: minio/minio
    container_name: lakehouse-minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      MINIO_DOMAIN: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    networks:
      default:
        aliases:
          - warehouse.minio
          - checkpoints.minio

  minio-mc:
    image: minio/mc
    depends_on:
      minio:
        condition: service_started
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      until (/usr/bin/mc alias set myminio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb --ignore-existing myminio/warehouse;
      /usr/bin/mc mb --ignore-existing myminio/checkpoints;
      echo 'Buckets created successfully';
      exit 0;
      "

  # ---------------------------
  # 2. Metadata Layer (Iceberg REST) - [NEW]
  # ---------------------------
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: lakehouse-catalog
    ports:
      - "8181:8181"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
    depends_on:
      - minio

  # ---------------------------
  # 3. Ingestion Layer (Kafka)
  # ---------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: lakehouse-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: lakehouse-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

  # ---------------------------
  # 4. Processing Layer (Spark)
  # ---------------------------
  spark:
    image: tabulario/spark-iceberg:latest
    container_name: lakehouse-spark
    mem_limit: 6g
    mem_reservation: 4g
    depends_on:
      - minio
      - kafka
      - iceberg-rest  # Spark 現在依賴 Catalog
    volumes:
      - ./src:/home/iceberg/local/src
      - ./spark-events:/home/iceberg/spark-events
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./ivy_cache:/root/.ivy2
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_ENDPOINT_URL: http://minio:9000
      AWS_ALLOW_HTTP: "true"
    ports:
      - "4040:4040"
      - "8888:8888"
      - "9090:8080"
      - "10000:10000"

  # ---------------------------
  # 5. Serving Layer (Trino) - [NEW]
  # ---------------------------
  trino:
    image: trinodb/trino:latest
    container_name: lakehouse-trino
    ports:
      - "8081:8080"  # 將 Trino UI 開在 8081 (避免跟 Spark UI 8080 衝突)
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      - iceberg-rest
      - minio
  
  airflow:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    container_name: lakehouse-airflow
    user: "0:0"
    ports:
      - "8082:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/home/iceberg/local/src
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SHOW_RECENT_STATS_FOR_DAG_RUNS=False
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && airflow scheduler & airflow webserver"
    depends_on:
      - spark
  grafana:
    image: grafana/grafana-enterprise
    container_name: lakehouse-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_INSTALL_PLUGINS=trino-datasource
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - trino